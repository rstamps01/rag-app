# download_models.py
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer

# Define cache directory within the container
CACHE_DIR = "/app/models_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# Get token from environment variable
hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
if not hf_token:
    print("Warning: HUGGING_FACE_HUB_TOKEN environment variable not set. Downloads for gated models may fail.")

# List of models to pre-download
llm_models = [
    "EleutherAI/gpt-j-6b",
    "meta-llama/Llama-2-7b-hf", # Re-added gated model
    "tiiuae/falcon-7b",
    "mistralai/Mistral-7B-v0.1" # Gated model, requires token
]

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"

print(f"--- Starting model download to {CACHE_DIR} ---")

# Download LLMs and Tokenizers
for model_name in llm_models:
    print(f"Downloading {model_name}...")
    try:
        # Pass token if available and model is gated
        is_gated = "meta-llama" in model_name or "mistralai" in model_name
        auth_token = hf_token if is_gated else None
        
        # Download model
        AutoModelForCausalLM.from_pretrained(model_name, cache_dir=CACHE_DIR, token=auth_token)
        # Download tokenizer
        AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR, token=auth_token)
        print(f"Successfully downloaded {model_name}.")
    except Exception as e:
        print(f"ERROR downloading {model_name}: {e}")

# Download Embedding Model
print(f"Downloading {embedding_model}...")
try:
    # Embedding model doesn't usually need a token, but pass if needed for specific private models
    SentenceTransformer(embedding_model, cache_folder=CACHE_DIR)
    print(f"Successfully downloaded {embedding_model}.")
except Exception as e:
    print(f"ERROR downloading {embedding_model}: {e}")

print("--- Model download script finished ---")

