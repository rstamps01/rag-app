from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    CSVLoader
)
import os
from typing import List, Dict, Any, Optional
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from app.services.gpu_accelerator import GPUAccelerator

class DocumentProcessor:
    """Service for processing documents and generating embeddings with RTX 5090 optimizations"""
    
    def __init__(self, use_gpu: bool = True):
        # Initialize GPU accelerator with RTX 5090 optimizations
        self.gpu_accelerator = GPUAccelerator()
        self.use_gpu = use_gpu and self.gpu_accelerator.cuda_available
        self.device = torch.device("cuda" if self.use_gpu else "cpu")
        self.is_rtx5090 = self.gpu_accelerator.is_ada_lovelace if self.use_gpu else False
        
        # Set up mixed precision for RTX 5090
        self.scaler = self.gpu_accelerator.setup_mixed_precision() if self.is_rtx5090 else None
        
        # Initialize embedding model
        self.tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        self.model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2").to(self.device)
        
        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # CUDA graph for embedding generation (RTX 5090 optimization)
        self.embedding_graph = None
        if self.is_rtx5090:
            # Create sample input for graph creation
            sample_text = ["This is a sample text for CUDA graph creation."]
            sample_input = self.tokenizer(sample_text, padding=True, truncation=True, return_tensors="pt").to(self.device)
            self.embedding_graph = self.gpu_accelerator.create_cuda_graph(self.model, sample_input)
    
    def process_document(self, file_path: str) -> tuple[List[Dict[str, Any]], List[List[float]]]:
        """
        Process a document file and generate embeddings
        
        Args:
            file_path: Path to the document file
            
        Returns:
            Tuple of (chunks, embeddings)
        """
        # Load document based on file type
        extension = os.path.splitext(file_path)[1].lower()
        
        if extension == ".pdf":
            loader = PyPDFLoader(file_path)
        elif extension == ".txt":
            loader = TextLoader(file_path)
        elif extension in [".docx", ".doc"]:
            loader = Docx2txtLoader(file_path)
        elif extension == ".csv":
            loader = CSVLoader(file_path)
        else:
            raise ValueError(f"Unsupported file type: {extension}")
        
        # Load and split document
        documents = loader.load()
        chunks = self.text_splitter.split_documents(documents)
        
        # Convert to dictionary format
        chunk_dicts = []
        for i, chunk in enumerate(chunks):
            chunk_dicts.append({
                "text": chunk.page_content,
                "metadata": {
                    "page": chunk.metadata.get("page", i),
                    "source": file_path
                }
            })
        
        # Generate embeddings
        embeddings = self.generate_embeddings([c["text"] for c in chunk_dicts])
        
        return chunk_dicts, embeddings
    
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a list of texts with RTX 5090 optimizations
        
        Args:
            texts: List of text strings
            
        Returns:
            List of embedding vectors
        """
        embeddings = []
        
        # Optimize batch size for RTX 5090
        batch_size = self.gpu_accelerator.optimize_batch_size(len(texts[0]) * 4, max_batch_size=64 if self.is_rtx5090 else 32)
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            # Tokenize
            inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt").to(self.device)
            
            # Use CUDA graph if available (RTX 5090 optimization)
            if self.embedding_graph and inputs.input_ids.shape == self.embedding_graph._input_shapes[0]:
                # Replay graph
                self.embedding_graph.replay()
                # Get output from graph (assuming model output is captured)
                # This part needs careful implementation based on how graph captures output
                # For now, we fall back to regular execution if graph replay is complex
                with torch.no_grad():
                    outputs = self.model(**inputs)
            else:
                # Use mixed precision for RTX 5090
                if self.is_rtx5090 and self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(**inputs)
                else:
                    with torch.no_grad():
                        outputs = self.model(**inputs)
            
            # Mean pooling
            attention_mask = inputs["attention_mask"]
            token_embeddings = outputs.last_hidden_state
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
            batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()
            
            embeddings.extend(batch_embeddings.tolist())
        
        return embeddings
