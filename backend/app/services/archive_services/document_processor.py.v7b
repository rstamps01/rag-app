# File Path: /backend/app/services/document_processor.py
# Enhanced version - Added department support while preserving all existing functionality

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
import torch
import numpy as np
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Updated Langchain Community Imports
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    CSVLoader
)

from app.core.config import settings
from app.core.pipeline_monitor import pipeline_monitor
from app.db.session import get_db
from app.crud.crud_document import create_document, get_document_by_filename, update_document

# FIXED: Import from documents.py instead of document.py to avoid circular import
from app.schemas.documents import DocumentCreate, DocumentUpdate

from sqlalchemy.orm import Session
import time
import uuid
import fitz  # PyMuPDF for PDF processing
import pytesseract
from PIL import Image
import io

logger = logging.getLogger(__name__)

# ENHANCED: Department validation constants
VALID_DEPARTMENTS = ["General", "IT", "HR", "Finance", "Legal"]

class DocumentProcessor:
    """Enhanced document processor with OCR capabilities, GPU optimization, and department support"""
    
    def __init__(self):
        self.embedding_model = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CONTEXT_WINDOW_SIZE // 4,
            chunk_overlap=200,
            length_function=len,
        )
        self.vector_client = None
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize embedding model and vector database client"""
        try:
            # Initialize embedding model with GPU optimization
            device = "cuda" if torch.cuda.is_available() and settings.ENABLE_GPU else "cpu"
            self.embedding_model = SentenceTransformer(
                'all-MiniLM-L6-v2',
                device=device
            )
            
            # Enable GPU optimizations if available
            if device == "cuda":
                torch.backends.cudnn.benchmark = True
                if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
                    torch.backends.cuda.enable_flash_sdp(True)
            
            # Initialize vector database client
            self.vector_client = QdrantClient(
                host=settings.QDRANT_HOST,
                port=settings.QDRANT_PORT
            )
            
            logger.info(f"Document processor initialized with device: {device}")
            
        except Exception as e:
            logger.error(f"Failed to initialize document processor: {str(e)}")
            raise

    def extract_text_with_ocr(self, file_path: str, file_type: str) -> str:
        """Extract text from documents using OCR when needed"""
        try:
            if file_type.lower() == 'pdf':
                return self._extract_pdf_with_ocr(file_path)
            elif file_type.lower() in ['png', 'jpg', 'jpeg', 'tiff', 'bmp']:
                return self._extract_image_text(file_path)
            else:
                # For other file types, use standard text extraction
                return self._extract_standard_text(file_path, file_type)
                
        except Exception as e:
            logger.error(f"OCR extraction failed for {file_path}: {str(e)}")
            return ""

    def _extract_pdf_with_ocr(self, file_path: str) -> str:
        """Extract text from PDF using PyMuPDF with OCR fallback"""
        text_content = []
        
        try:
            doc = fitz.open(file_path)
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # Try standard text extraction first
                text = page.get_text()
                
                # If no text found or very little text, use OCR
                if len(text.strip()) < 50:
                    # Convert page to image and apply OCR
                    pix = page.get_pixmap()
                    img_data = pix.tobytes("png")
                    img = Image.open(io.BytesIO(img_data))
                    
                    # Apply OCR
                    ocr_text = pytesseract.image_to_string(
                        img, 
                        lang=settings.OCR_LANGUAGE
                    )
                    text_content.append(ocr_text)
                else:
                    text_content.append(text)
            
            doc.close()
            return "\n".join(text_content)
            
        except Exception as e:
            logger.error(f"PDF OCR extraction failed: {str(e)}")
            return ""

    def _extract_image_text(self, file_path: str) -> str:
        """Extract text from image files using OCR"""
        try:
            img = Image.open(file_path)
            text = pytesseract.image_to_string(img, lang=settings.OCR_LANGUAGE)
            return text
        except Exception as e:
            logger.error(f"Image OCR extraction failed: {str(e)}")
            return ""

    def _extract_standard_text(self, file_path: str, file_type: str) -> str:
        """Extract text using standard document loaders"""
        try:
            if file_type.lower() == 'pdf':
                loader = PyPDFLoader(file_path)
            elif file_type.lower() == 'txt':
                loader = TextLoader(file_path)
            elif file_type.lower() in ['doc', 'docx']:
                loader = Docx2txtLoader(file_path)
            elif file_type.lower() == 'csv':
                loader = CSVLoader(file_path)
            else:
                logger.warning(f"Unsupported file type: {file_type}")
                return ""
            
            documents = loader.load()
            return "\n".join([doc.page_content for doc in documents])
            
        except Exception as e:
            logger.error(f"Standard text extraction failed: {str(e)}")
            return ""

    def process_document(self, file_path: str, filename: str, content_type: str, department: str = "General") -> Dict[str, Any]:
        """
        ENHANCED: Process a document and store it in the vector database with department support
        
        Args:
            file_path: Path to the document file
            filename: Original filename
            content_type: MIME type of the file
            department: Department categorization for the document
        """
        start_time = time.time()
        
        # ENHANCED: Validate department
        if department not in VALID_DEPARTMENTS:
            logger.warning(f"Invalid department '{department}', defaulting to 'General'")
            department = "General"
        
        try:
            # Extract text with OCR support
            text_content = self.extract_text_with_ocr(file_path, content_type)
            
            if not text_content.strip():
                raise ValueError("No text content extracted from document")
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text_content)
            
            # Generate embeddings
            embeddings = self.embedding_model.encode(chunks)
            
            # Store in vector database with department metadata
            collection_name = "documents"
            points = []
            
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                point_id = f"{filename}_{i}"
                points.append(models.PointStruct(
                    id=point_id,
                    vector=embedding.tolist(),
                    payload={
                        "filename": filename,
                        "chunk_index": i,
                        "content": chunk,
                        "content_type": content_type,
                        "department": department  # ENHANCED: Store department in vector DB
                    }
                ))
            
            # Upsert points to vector database
            self.vector_client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            processing_time = time.time() - start_time
            
            # Log processing metrics with department info
            pipeline_monitor.log_document_processed(
                filename=filename,
                processing_time=processing_time,
                chunk_count=len(chunks),
                success=True,
                metadata={"department": department}  # ENHANCED: Include department in metrics
            )
            
            return {
                "status": "completed",
                "chunks_processed": len(chunks),
                "processing_time": processing_time,
                "text_length": len(text_content),
                "department": department  # ENHANCED: Return department info
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            # Log processing failure
            pipeline_monitor.log_document_processed(
                filename=filename,
                processing_time=processing_time,
                chunk_count=0,
                success=False,
                error=error_msg,
                metadata={"department": department}
            )
            
            logger.error(f"Document processing failed for {filename} (dept: {department}): {error_msg}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "processing_time": processing_time,
                "department": department
            }

# Global processor instance 
document_processor = DocumentProcessor()

def process_and_store_document(file_path: str, filename: str, content_type: str, db: Session, department: str = "General") -> Dict[str, Any]:
    """
    ENHANCED: Process and store a document in both vector database and PostgreSQL with department support
    
    Args:
        file_path: Path to the uploaded file
        filename: Original filename
        content_type: MIME type of the file
        db: Database session
        department: Department categorization (General, IT, HR, Finance, Legal)
        
    Returns:
        Dictionary with processing results including department info
    """
    # ENHANCED: Validate department parameter
    if department not in VALID_DEPARTMENTS:
        logger.warning(f"Invalid department '{department}', defaulting to 'General'")
        department = "General"
    
    logger.info(f"Processing document {filename} for department: {department}")
    
    try:
        # ENHANCED: Create document record in PostgreSQL with department
        document_create = DocumentCreate(
            filename=filename,
            content_type=content_type,
            department=department  # ENHANCED: Store department in PostgreSQL
        )
        
        db_document = create_document(db, document_create)
        
        # ENHANCED: Process document with vector database storage and department
        result = document_processor.process_document(file_path, filename, content_type, department)
        
        # Update document record with processing results
        document_update = DocumentUpdate(
            status=result["status"],
            path=file_path,
            error_message=result.get("error"),
            department=department  # ENHANCED: Ensure department is stored
        )
        
        update_document(db, db_document, document_update)
        
        return {
            "document_id": db_document.id,
            "processing_result": result,
            "department": department  # ENHANCED: Return department info
        }
        
    except Exception as e:
        logger.error(f"Failed to process and store document {filename} (dept: {department}): {str(e)}")
        raise

def get_valid_departments() -> List[str]:
    """
    ENHANCED: Get list of valid departments for validation
    
    Returns:
        List of valid department names
    """
    return VALID_DEPARTMENTS.copy()

def filter_documents_by_department(department: str, limit: int = 100) -> List[Dict[str, Any]]:
    """
    ENHANCED: Filter documents by department from vector database
    
    Args:
        department: Department to filter by
        limit: Maximum number of documents to return
        
    Returns:
        List of document metadata filtered by department
    """
    if department not in VALID_DEPARTMENTS:
        logger.warning(f"Invalid department '{department}' for filtering")
        return []
    
    try:
        # Query vector database for documents in specific department
        search_result = document_processor.vector_client.scroll(
            collection_name="documents",
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="department",
                        match=models.MatchValue(value=department)
                    )
                ]
            ),
            limit=limit,
            with_payload=True
        )
        
        # Extract unique documents (group by filename)
        documents = {}
        for point in search_result[0]:
            filename = point.payload["filename"]
            if filename not in documents:
                documents[filename] = {
                    "filename": filename,
                    "department": point.payload["department"],
                    "content_type": point.payload["content_type"],
                    "chunk_count": 1
                }
            else:
                documents[filename]["chunk_count"] += 1
        
        return list(documents.values())
        
    except Exception as e:
        logger.error(f"Failed to filter documents by department {department}: {str(e)}")
        return []
