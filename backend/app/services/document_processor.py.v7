# File Path: /backend/app/services/document_processor.py
# Fixed version - corrected import paths to avoid circular import

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
import torch
import numpy as np
from qdrant_client import QdrantClient, models
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Updated Langchain Community Imports
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    CSVLoader
)

from app.core.config import settings
from app.core.pipeline_monitor import pipeline_monitor
from app.db.session import get_db
from app.crud.crud_document import create_document, get_document_by_filename, update_document

# FIXED: Import from documents.py instead of document.py to avoid circular import
from app.schemas.documents import DocumentCreate, DocumentUpdate

from sqlalchemy.orm import Session
import time
import uuid
import fitz  # PyMuPDF for PDF processing
import pytesseract
from PIL import Image
import io

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """Enhanced document processor with OCR capabilities and GPU optimization"""
    
    def __init__(self):
        self.embedding_model = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CONTEXT_WINDOW_SIZE // 4,
            chunk_overlap=200,
            length_function=len,
        )
        self.vector_client = None
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize embedding model and vector database client"""
        try:
            # Initialize embedding model with GPU optimization
            device = "cuda" if torch.cuda.is_available() and settings.ENABLE_GPU else "cpu"
            self.embedding_model = SentenceTransformer(
                'all-MiniLM-L6-v2',
                device=device
            )
            
            # Enable GPU optimizations if available
            if device == "cuda":
                torch.backends.cudnn.benchmark = True
                if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
                    torch.backends.cuda.enable_flash_sdp(True)
            
            # Initialize vector database client
            self.vector_client = QdrantClient(
                host=settings.QDRANT_HOST,
                port=settings.QDRANT_PORT
            )
            
            logger.info(f"Document processor initialized with device: {device}")
            
        except Exception as e:
            logger.error(f"Failed to initialize document processor: {str(e)}")
            raise

    def extract_text_with_ocr(self, file_path: str, file_type: str) -> str:
        """Extract text from documents using OCR when needed"""
        try:
            if file_type.lower() == 'pdf':
                return self._extract_pdf_with_ocr(file_path)
            elif file_type.lower() in ['png', 'jpg', 'jpeg', 'tiff', 'bmp']:
                return self._extract_image_text(file_path)
            else:
                # For other file types, use standard text extraction
                return self._extract_standard_text(file_path, file_type)
                
        except Exception as e:
            logger.error(f"OCR extraction failed for {file_path}: {str(e)}")
            return ""

    def _extract_pdf_with_ocr(self, file_path: str) -> str:
        """Extract text from PDF using PyMuPDF with OCR fallback"""
        text_content = []
        
        try:
            doc = fitz.open(file_path)
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # Try standard text extraction first
                text = page.get_text()
                
                # If no text found or very little text, use OCR
                if len(text.strip()) < 50:
                    # Convert page to image and apply OCR
                    pix = page.get_pixmap()
                    img_data = pix.tobytes("png")
                    img = Image.open(io.BytesIO(img_data))
                    
                    # Apply OCR
                    ocr_text = pytesseract.image_to_string(
                        img, 
                        lang=settings.OCR_LANGUAGE
                    )
                    text_content.append(ocr_text)
                else:
                    text_content.append(text)
            
            doc.close()
            return "\n".join(text_content)
            
        except Exception as e:
            logger.error(f"PDF OCR extraction failed: {str(e)}")
            return ""

    def _extract_image_text(self, file_path: str) -> str:
        """Extract text from image files using OCR"""
        try:
            img = Image.open(file_path)
            text = pytesseract.image_to_string(img, lang=settings.OCR_LANGUAGE)
            return text
        except Exception as e:
            logger.error(f"Image OCR extraction failed: {str(e)}")
            return ""

    def _extract_standard_text(self, file_path: str, file_type: str) -> str:
        """Extract text using standard document loaders"""
        try:
            if file_type.lower() == 'pdf':
                loader = PyPDFLoader(file_path)
            elif file_type.lower() == 'txt':
                loader = TextLoader(file_path)
            elif file_type.lower() in ['doc', 'docx']:
                loader = Docx2txtLoader(file_path)
            elif file_type.lower() == 'csv':
                loader = CSVLoader(file_path)
            else:
                logger.warning(f"Unsupported file type: {file_type}")
                return ""
            
            documents = loader.load()
            return "\n".join([doc.page_content for doc in documents])
            
        except Exception as e:
            logger.error(f"Standard text extraction failed: {str(e)}")
            return ""

    def process_document(self, file_path: str, filename: str, content_type: str) -> Dict[str, Any]:
        """Process a document and store it in the vector database"""
        start_time = time.time()
        
        try:
            # Extract text with OCR support
            text_content = self.extract_text_with_ocr(file_path, content_type)
            
            if not text_content.strip():
                raise ValueError("No text content extracted from document")
            
            # Split text into chunks
            chunks = self.text_splitter.split_text(text_content)
            
            # Generate embeddings
            embeddings = self.embedding_model.encode(chunks)
            
            # Store in vector database
            collection_name = "documents"
            points = []
            
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                point_id = f"{filename}_{i}"
                points.append(models.PointStruct(
                    id=point_id,
                    vector=embedding.tolist(),
                    payload={
                        "filename": filename,
                        "chunk_index": i,
                        "content": chunk,
                        "content_type": content_type
                    }
                ))
            
            # Upsert points to vector database
            self.vector_client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            processing_time = time.time() - start_time
            
            # Log processing metrics
            pipeline_monitor.log_document_processed(
                filename=filename,
                processing_time=processing_time,
                chunk_count=len(chunks),
                success=True
            )
            
            return {
                "status": "completed",
                "chunks_processed": len(chunks),
                "processing_time": processing_time,
                "text_length": len(text_content)
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            
            # Log processing failure
            pipeline_monitor.log_document_processed(
                filename=filename,
                processing_time=processing_time,
                chunk_count=0,
                success=False,
                error=error_msg
            )
            
            logger.error(f"Document processing failed for {filename}: {error_msg}")
            
            return {
                "status": "failed",
                "error": error_msg,
                "processing_time": processing_time
            }

# Global processor instance 
document_processor = DocumentProcessor()

def process_and_store_document(file_path: str, filename: str, content_type: str, db: Session) -> Dict[str, Any]:
    """
    Process and store a document in both vector database and PostgreSQL
    
    Args:
        file_path: Path to the uploaded file
        filename: Original filename
        content_type: MIME type of the file
        db: Database session
        
    Returns:
        Dictionary with processing results
    """
    try:
        # Create document record in PostgreSQL
        document_create = DocumentCreate(
            filename=filename,
            content_type=content_type
        )
        
        db_document = create_document(db, document_create)
        
        # Process document with vector database storage
        result = document_processor.process_document(file_path, filename, content_type)
        
        # Update document record with processing results
        document_update = DocumentUpdate(
            status=result["status"],
            path=file_path,
            error_message=result.get("error")
        )
        
        update_document(db, db_document, document_update)
        
        return {
            "document_id": db_document.id,
            "processing_result": result
        }
        
    except Exception as e:
        logger.error(f"Failed to process and store document {filename}: {str(e)}")
        raise
