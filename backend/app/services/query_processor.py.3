# File Path: /home/ubuntu/rag_app_v2/rag-app/backend/app/services/query_processor.py
import logging
from time import time
from fastapi import HTTPException
import torch
from typing import List, Dict, Any

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

from ..core.config import settings
from ..schemas.query import QueryResponse, SourceDocument # Use schemas defined earlier
from .document_processor import embedding_model, qdrant_client, device, QDRANT_COLLECTION_NAME # Reuse components

logger = logging.getLogger(__name__)

# --- Configuration ---
LLM_MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.1" # Or another Mistral variant
MAX_CONTEXT_LENGTH = 4096 # Adjust based on model limits and desired context window
MAX_NEW_TOKENS = 512 # Max tokens for LLM generation

# --- LLM Initialization ---
llm_pipeline = None
if device == "cuda":
    try:
        logger.info(f"Loading LLM model: {LLM_MODEL_NAME} onto GPU ({device})")
        # Load model with optimizations for GPU (e.g., 8-bit quantization if needed)
        # Note: 8-bit requires bitsandbytes and accelerate
        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_NAME,
            # load_in_8bit=True, # Uncomment for 8-bit quantization
            torch_dtype=torch.float16, # Use float16 for faster inference
            device_map="auto" # Automatically distribute across available GPUs
        )
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
        
        # Create a text generation pipeline
        llm_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            torch_dtype=torch.float16,
            device_map="auto",
            max_new_tokens=MAX_NEW_TOKENS
        )
        logger.info(f"LLM pipeline created successfully for {LLM_MODEL_NAME} on GPU.")
    except Exception as e:
        logger.error(f"Failed to load LLM model {LLM_MODEL_NAME} on GPU: {e}", exc_info=True)
        llm_pipeline = None
elif settings.USE_GPU:
     logger.warning(f"GPU requested but not available. LLM ({LLM_MODEL_NAME}) will not be loaded.")
else:
    # Optional: Load on CPU if GPU is disabled (will be very slow for Mistral 7B)
    logger.warning(f"GPU disabled. Loading LLM ({LLM_MODEL_NAME}) on CPU is not recommended for performance.")
    # try:
    #     logger.info(f"Loading LLM model: {LLM_MODEL_NAME} onto CPU")
    #     model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME)
    #     tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
    #     llm_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, device=-1) # device=-1 for CPU
    #     logger.info(f"LLM pipeline created successfully for {LLM_MODEL_NAME} on CPU.")
    # except Exception as e:
    #     logger.error(f"Failed to load LLM model {LLM_MODEL_NAME} on CPU: {e}", exc_info=True)
    #     llm_pipeline = None

# --- Core Query Function ---
async def process_query(
    query_text: str, 
    department: str
) -> QueryResponse:
    """
    Processes a query using the RAG pipeline: embed, search, generate.
    """
    if not embedding_model or not qdrant_client:
        raise HTTPException(status_code=503, detail="Embedding model or Qdrant client not available.")
    if not llm_pipeline:
         raise HTTPException(status_code=503, detail="LLM pipeline not available.")

    start_time = time()

    # 1. Generate Query Embedding
    logger.info(f"Generating embedding for query: \"{query_text}\"")
    with torch.cuda.amp.autocast() if device == "cuda" else torch.no_grad():
        query_vector = embedding_model.encode(query_text, convert_to_numpy=True).tolist()

    # 2. Search Vector Store (Qdrant) with Department Filter
    logger.info(f"Searching Qdrant collection \"{QDRANT_COLLECTION_NAME}\" for department \"{department}\"")
    search_result = qdrant_client.search(
        collection_name=QDRANT_COLLECTION_NAME,
        query_vector=query_vector,
        query_filter=models.Filter(
            must=[
                models.FieldCondition(
                    key="department",
                    match=models.MatchValue(value=department)
                )
            ]
        ),
        limit=5 # Retrieve top 5 relevant chunks
    )
    logger.info(f"Retrieved {len(search_result)} results from Qdrant.")

    # 3. Prepare Context and Source Documents
    context = ""
    sources = []
    if search_result:
        for hit in search_result:
            if hit.payload:
                context += hit.payload.get("text", "") + "\n\n" # Add chunk text to context
                sources.append(SourceDocument(
                    document_id=hit.payload.get("document_id", "unknown"),
                    document_name=f"doc_{hit.payload.get('document_id', 'unknown')}", # CORRECTED f-string quote usage
                    relevance_score=hit.score,
                    content_snippet=hit.payload.get("text", "")[:200] + "..." # Add snippet
                ))
        # Truncate context if it exceeds model limits (simple truncation)
        # A more sophisticated approach might prioritize higher-scoring chunks
        context = context[:MAX_CONTEXT_LENGTH] 

    # 4. Prepare Prompt for LLM (Mistral Instruct format)
    # Reference: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1#instruction-format
    prompt = f"<s>[INST] Use the following context to answer the question. If the answer is not in the context, say you don\'t know.\n\nContext:\n{context}\n\nQuestion: {query_text} [/INST]" 

    # 5. Call LLM Pipeline
    logger.info(f"Calling LLM pipeline for query: \"{query_text}\"")
    answer = "Error: LLM processing failed." # Default error message
    try: # Line 126
        # Use autocast for mixed precision during generation if applicable
        with torch.cuda.amp.autocast() if device == "cuda" else torch.no_grad():
            llm_output = llm_pipeline(prompt)
        
        # Extract the generated text (output format depends on pipeline)
        if llm_output and isinstance(llm_output, list) and len(llm_output) > 0:
            # Assuming the pipeline returns a list of dicts like [{'generated_text': '...prompt...answer'}]
            full_generated_text = llm_output[0]['generated_text'] # CORRECTED: Removed unnecessary escaping
            # Extract only the answer part after the [/INST] tag
            answer = full_generated_text.split("[/INST]")[-1].strip()
        else:
            logger.error(f"LLM pipeline returned unexpected output: {llm_output}")
            answer = "Error: Failed to get response from LLM."
            
    except Exception as e: # Corresponding except block for try on line 126
        logger.error(f"Error during LLM generation: {e}", exc_info=True)
        answer = f"Error: Exception during LLM generation - {e}"

    logger.info(f"LLM generation completed.")
    processing_time = time() - start_time

    # 6. Construct Final Response
    final_response = QueryResponse(
        query=query_text,
        response=answer,
        model=LLM_MODEL_NAME,
        sources=sources,
        processing_time=round(processing_time, 2),
        gpu_accelerated=(device == "cuda")
    )

    return final_response
# Ensure a newline at the end of the file

