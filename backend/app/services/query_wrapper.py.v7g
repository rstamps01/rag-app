"""Enhanced query processing wrapper with singleton pattern, memory management, and corrected schema integration"""
import logging
import time
import psutil
import torch
from typing import Optional, List, Any, Dict
from sqlalchemy.orm import Session
from datetime import datetime

# Import the correct schema with proper field names
from app.schemas.query import QueryResponse, SourceDocument, QueryHistoryCreate
from app.services.llm_service import get_llm_service
from app.services.gpu_accelerator import GPUAccelerator

logger = logging.getLogger(__name__)

# Global instances (singleton pattern)
_gpu_accelerator = None
_query_processor = None

def get_gpu_accelerator() -> GPUAccelerator:
    """Get or create GPU accelerator instance"""
    global _gpu_accelerator
    if _gpu_accelerator is None:
        _gpu_accelerator = GPUAccelerator()
        logger.info("GPU accelerator initialized")
    return _gpu_accelerator

def get_query_processor():
    """Get or create query processor instance"""
    global _query_processor
    if _query_processor is None:
        try:
            from app.services.query_processor import QueryProcessor
            _query_processor = QueryProcessor()
            logger.info("Query processor initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize query processor: {e}")
            _query_processor = None
    return _query_processor

def check_system_resources() -> Dict[str, Any]:
    """Check system resources before processing query"""
    memory = psutil.virtual_memory()
    gpu_info = {"available": False}
    
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "memory_allocated_gb": torch.cuda.memory_allocated(0) / 1024**3,
            "memory_total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
        }
    
    return {
        "system_memory_percent": memory.percent,
        "system_memory_available_gb": memory.available / 1024**3,
        "gpu": gpu_info
    }

def cleanup_memory():
    """Cleanup memory before processing query"""
    try:
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Force garbage collection
        import gc
        gc.collect()
        
        logger.info("Memory cleanup completed")
    except Exception as e:
        logger.warning(f"Memory cleanup failed: {e}")

async def process_query(
    db: Session,
    query_text: str,
    department: str = "General",
    user_id: Optional[int] = None
) -> QueryResponse:
    """
    Process a query through the RAG pipeline with memory management and error handling
    FIXED: Corrected schema field names and method signatures
    """
    start_time = time.time()
    
    try:
        logger.info(f"Processing query: {query_text[:100]}...")
        
        # Check system resources before processing
        resources = check_system_resources()
        logger.info(f"System memory: {resources['system_memory_percent']:.1f}%, GPU available: {resources['gpu']['available']}")
        
        # If memory usage is high, perform cleanup
        if resources['system_memory_percent'] > 85:
            logger.warning("High memory usage detected, performing cleanup")
            cleanup_memory()
            
            # Check again after cleanup
            resources = check_system_resources()
            if resources['system_memory_percent'] > 90:
                return QueryResponse(
                    query=query_text,
                    response="System memory critically low. Please try again in a moment.",  # FIXED: 'response' not 'answer'
                    model="system-error",                                                    # FIXED: 'model' not 'model_used'
                    sources=[],
                    processing_time=time.time() - start_time,
                    gpu_accelerated=False,
                    query_history_id=None
                )
        
        # Get or initialize services (singleton pattern prevents multiple loading)
        try:
            llm_service = get_llm_service()  # This will reuse existing instance
            query_processor = get_query_processor()
            
            if query_processor is None:
                # Fallback to direct LLM service if query processor fails
                logger.warning("Query processor unavailable, using direct LLM service")
                return await process_query_direct(db, query_text, department, user_id, llm_service, start_time)
                
        except Exception as e:
            logger.error(f"Failed to initialize services: {e}")
            return QueryResponse(
                query=query_text,
                response=f"Service initialization failed: {str(e)}",                       # FIXED: 'response' not 'answer'
                model="initialization-error",                                              # FIXED: 'model' not 'model_used'
                sources=[],
                processing_time=time.time() - start_time,
                gpu_accelerated=False,
                query_history_id=None
            )
        
        # FIXED: Process the query using correct parameter names
        try:
            response = query_processor.process_query(
                query=query_text,                    # CORRECT: 'query' parameter
                department_filter=department,        # CORRECT: 'department_filter' not 'department'
                user_id=user_id,
                db=db
            )
            
            # Log successful processing
            processing_time = time.time() - start_time
            logger.info(f"Query processed successfully in {processing_time:.2f}s")
            
            # Update response with actual processing time
            response.processing_time = processing_time
            
            return response
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            
            # Fallback to direct LLM service
            logger.info("Attempting fallback to direct LLM service")
            return await process_query_direct(db, query_text, department, user_id, llm_service, start_time)
            
    except Exception as e:
        logger.error(f"Unexpected error in process_query: {e}", exc_info=True)
        return QueryResponse(
            query=query_text,
            response=f"Unexpected error: {str(e)}",                                       # FIXED: 'response' not 'answer'
            model="unexpected-error",                                                     # FIXED: 'model' not 'model_used'
            sources=[],
            processing_time=time.time() - start_time,
            gpu_accelerated=False,
            query_history_id=None
        )

async def process_query_direct(
    db: Session,
    query_text: str,
    department: str,
    user_id: Optional[int],
    llm_service,
    start_time: float
) -> QueryResponse:
    """
    Direct query processing using LLM service as fallback
    FIXED: Corrected method signature and schema field names
    """
    try:
        logger.info("Processing query using direct LLM service")
        
        # FIXED: Generate response using correct method signature
        response_text = llm_service.generate_response(
            prompt=query_text,                       # FIXED: 'prompt' not 'query'
            context="",                              # No RAG context in fallback mode
            max_new_tokens=512
        )
        
        # Check if GPU was used
        gpu_accelerated = torch.cuda.is_available() and hasattr(llm_service, 'device') and llm_service.device.type == "cuda"
        
        processing_time = time.time() - start_time
        
        # Store query history if database session provided
        if db and user_id:
            try:
                from app.crud.crud_query_history import create_query_history
                
                query_history = QueryHistoryCreate(
                    query_text=query_text,
                    response_text=response_text,
                    llm_model_used="mistralai/Mistral-7B-Instruct-v0.2",
                    sources_retrieved=[],
                    processing_time_ms=int(processing_time * 1000),
                    department_filter=department,
                    gpu_accelerated=gpu_accelerated
                )
                create_query_history(db, query_history)
            except Exception as e:
                logger.warning(f"Failed to store query history: {str(e)}")
        
        # FIXED: Return QueryResponse with correct field names
        return QueryResponse(
            query=query_text,
            response=response_text,                                                       # FIXED: 'response' not 'answer'
            model="mistralai/Mistral-7B-Instruct-v0.2",                                 # FIXED: 'model' not 'model_used'
            sources=[],  # No sources in direct mode
            processing_time=processing_time,
            gpu_accelerated=gpu_accelerated,
            query_history_id=None
        )
        
    except Exception as e:
        logger.error(f"Direct query processing failed: {e}")
        return QueryResponse(
            query=query_text,
            response=f"Direct processing failed: {str(e)}",                             # FIXED: 'response' not 'answer'
            model="direct-error",                                                        # FIXED: 'model' not 'model_used'
            sources=[],
            processing_time=time.time() - start_time,
            gpu_accelerated=False,
            query_history_id=None
        )

def create_source_document_from_result(result: Dict[str, Any], index: int = 0) -> SourceDocument:
    """
    Create a SourceDocument from vector database search result
    FIXED: Proper field mapping from vector DB to schema
    """
    filename = result.get("filename", f"doc_{index + 1}")
    content = result.get("content", "")
    
    return SourceDocument(
        document_id=filename,                           # Map filename to document_id
        document_name=filename,                         # Map filename to document_name
        relevance_score=result.get("score", 0.0),      # Map score to relevance_score
        content_snippet=content[:200] + "..." if len(content) > 200 else content  # Truncate content
    )

def get_system_status() -> Dict[str, Any]:
    """Get current system status for monitoring"""
    try:
        resources = check_system_resources()
        llm_service = get_llm_service()
        
        status = {
            "system_resources": resources,
            "llm_service_initialized": llm_service is not None,
            "query_processor_initialized": get_query_processor() is not None,
            "gpu_accelerator_initialized": _gpu_accelerator is not None
        }
        
        if llm_service and hasattr(llm_service, 'get_memory_usage'):
            status["llm_memory_usage"] = llm_service.get_memory_usage()
        
        return status
        
    except Exception as e:
        logger.error(f"Failed to get system status: {e}")
        return {"error": str(e)}

# Health check function
def health_check() -> Dict[str, Any]:
    """Perform health check on all components"""
    try:
        status = get_system_status()
        
        # Check if critical components are healthy
        healthy = (
            status.get("system_resources", {}).get("system_memory_percent", 100) < 90 and
            status.get("llm_service_initialized", False)
        )
        
        return {
            "healthy": healthy,
            "status": status,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "healthy": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# FIXED: Backward compatibility function with correct schema
def create_query_response_from_legacy(
    query: str,
    answer: str,
    model_used: str = "mistral-7b",
    sources: List[Dict[str, Any]] = None,
    processing_time: float = 0.0,
    gpu_accelerated: bool = False
) -> QueryResponse:
    """
    Create QueryResponse from legacy format (for backward compatibility)
    FIXED: Maps legacy field names to correct schema field names
    """
    # Convert legacy sources to SourceDocument objects
    source_docs = []
    if sources:
        for i, source in enumerate(sources):
            source_doc = create_source_document_from_result(source, i)
            source_docs.append(source_doc)
    
    return QueryResponse(
        query=query,
        response=answer,                                # Map 'answer' to 'response'
        model=model_used,                              # Map 'model_used' to 'model'
        sources=source_docs,
        processing_time=processing_time,
        gpu_accelerated=gpu_accelerated,
        query_history_id=None
    )

# Export functions for module-level access
__all__ = [
    'process_query',
    'get_system_status', 
    'health_check',
    'create_source_document_from_result',
    'create_query_response_from_legacy'
]
