"""Enhanced query processing wrapper with singleton pattern and memory management"""
import logging
import time
import psutil
import torch
from typing import Optional, List, Any, Dict
from sqlalchemy.orm import Session
from datetime import datetime

# Import the correct schema
from app.schemas.query import QueryResponse
from app.services.llm_service import get_llm_service
from app.services.gpu_accelerator import GPUAccelerator

logger = logging.getLogger(__name__)

# Global instances (singleton pattern)
_gpu_accelerator = None
_query_processor = None

def get_gpu_accelerator() -> GPUAccelerator:
    """Get or create GPU accelerator instance"""
    global _gpu_accelerator
    if _gpu_accelerator is None:
        _gpu_accelerator = GPUAccelerator()
        logger.info("GPU accelerator initialized")
    return _gpu_accelerator

def get_query_processor():
    """Get or create query processor instance"""
    global _query_processor
    if _query_processor is None:
        try:
            from app.services.query_processor import QueryProcessor
            _query_processor = QueryProcessor()
            logger.info("Query processor initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize query processor: {e}")
            _query_processor = None
    return _query_processor

def check_system_resources() -> Dict[str, Any]:
    """Check system resources before processing query"""
    memory = psutil.virtual_memory()
    gpu_info = {"available": False}
    
    if torch.cuda.is_available():
        gpu_info = {
            "available": True,
            "memory_allocated_gb": torch.cuda.memory_allocated(0) / 1024**3,
            "memory_total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3
        }
    
    return {
        "system_memory_percent": memory.percent,
        "system_memory_available_gb": memory.available / 1024**3,
        "gpu": gpu_info
    }

def cleanup_memory():
    """Cleanup memory before processing query"""
    try:
        # Clear GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Force garbage collection
        import gc
        gc.collect()
        
        logger.info("Memory cleanup completed")
    except Exception as e:
        logger.warning(f"Memory cleanup failed: {e}")

async def process_query(
    db: Session,
    query_text: str,
    department: str = "General",
    user_id: Optional[int] = None
) -> QueryResponse:
    """
    Process a query through the RAG pipeline with memory management and error handling
    """
    start_time = time.time()
    
    try:
        logger.info(f"Processing query: {query_text[:100]}...")
        
        # Check system resources before processing
        resources = check_system_resources()
        logger.info(f"System memory: {resources['system_memory_percent']:.1f}%, GPU available: {resources['gpu']['available']}")
        
        # If memory usage is high, perform cleanup
        if resources['system_memory_percent'] > 85:
            logger.warning("High memory usage detected, performing cleanup")
            cleanup_memory()
            
            # Check again after cleanup
            resources = check_system_resources()
            if resources['system_memory_percent'] > 90:
                return QueryResponse(
                    query=query_text,
                    response="System memory critically low. Please try again in a moment.",
                    model="system-error",
                    sources=[],
                    processing_time=time.time() - start_time,
                    gpu_accelerated=False,
                    query_history_id=None
                )
        
        # Get or initialize services (singleton pattern prevents multiple loading)
        try:
            llm_service = get_llm_service()  # This will reuse existing instance
            query_processor = get_query_processor()
            
            if query_processor is None:
                # Fallback to direct LLM service if query processor fails
                logger.warning("Query processor unavailable, using direct LLM service")
                return await process_query_direct(db, query_text, department, user_id, llm_service, start_time)
                
        except Exception as e:
            logger.error(f"Failed to initialize services: {e}")
            return QueryResponse(
                query=query_text,
                response=f"Service initialization failed: {str(e)}",
                model="initialization-error",
                sources=[],
                processing_time=time.time() - start_time,
                gpu_accelerated=False,
                query_history_id=None
            )
        
        # Process the query using the singleton query processor
        try:
            response = query_processor.process_query(
                query=query_text,
                department_filter=department,
                user_id=user_id,
                db=db
            )
            
            # Log successful processing
            processing_time = time.time() - start_time
            logger.info(f"Query processed successfully in {processing_time:.2f}s")
            
            # Update response with actual processing time
            response.processing_time = processing_time
            
            return response
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            
            # Fallback to direct LLM service
            logger.info("Attempting fallback to direct LLM service")
            return await process_query_direct(db, query_text, department, user_id, llm_service, start_time)
            
    except Exception as e:
        logger.error(f"Unexpected error in process_query: {e}", exc_info=True)
        return QueryResponse(
            query=query_text,
            response=f"Unexpected error: {str(e)}",
            model="unexpected-error",
            sources=[],
            processing_time=time.time() - start_time,
            gpu_accelerated=False,
            query_history_id=None
        )

async def process_query_direct(
    db: Session,
    query_text: str,
    department: str,
    user_id: Optional[int],
    llm_service,
    start_time: float
) -> QueryResponse:
    """
    Direct query processing using LLM service as fallback
    """
    try:
        logger.info("Processing query using direct LLM service")
        
        # Generate response using LLM service
        response_text = llm_service.generate_response(
            prompt=query_text,
            context="",  # No RAG context in fallback mode
            max_new_tokens=512
        )
        
        # Check if GPU was used
        gpu_accelerated = torch.cuda.is_available() and llm_service.device.type == "cuda"
        
        processing_time = time.time() - start_time
        
        # Store query history if database session provided
        if db and user_id:
            try:
                from app.crud.crud_query_history import create_query_history
                from app.schemas.query import QueryHistoryCreate
                
                query_history = QueryHistoryCreate(
                    user_id=user_id,
                    query_text=query_text,
                    response_text=response_text,
                    llm_model_used="mistralai/Mistral-7B-Instruct-v0.2",
                    sources_retrieved=[],
                    processing_time_ms=int(processing_time * 1000),
                    department_filter=department,
                    gpu_accelerated=gpu_accelerated
                )
                create_query_history(db, query_history)
            except Exception as e:
                logger.warning(f"Failed to store query history: {str(e)}")
        
        return QueryResponse(
            query=query_text,
            response=response_text,
            model="mistralai/Mistral-7B-Instruct-v0.2",
            sources=[],  # No sources in direct mode
            processing_time=processing_time,
            gpu_accelerated=gpu_accelerated,
            query_history_id=None
        )
        
    except Exception as e:
        logger.error(f"Direct query processing failed: {e}")
        return QueryResponse(
            query=query_text,
            response=f"Direct processing failed: {str(e)}",
            model="direct-error",
            sources=[],
            processing_time=time.time() - start_time,
            gpu_accelerated=False,
            query_history_id=None
        )

def get_system_status() -> Dict[str, Any]:
    """Get current system status for monitoring"""
    try:
        resources = check_system_resources()
        llm_service = get_llm_service()
        
        status = {
            "system_resources": resources,
            "llm_service_initialized": llm_service is not None,
            "query_processor_initialized": get_query_processor() is not None,
            "gpu_accelerator_initialized": _gpu_accelerator is not None
        }
        
        if llm_service:
            status["llm_memory_usage"] = llm_service.get_memory_usage()
        
        return status
        
    except Exception as e:
        logger.error(f"Failed to get system status: {e}")
        return {"error": str(e)}

# Health check function
def health_check() -> Dict[str, Any]:
    """Perform health check on all components"""
    try:
        status = get_system_status()
        
        # Check if critical components are healthy
        healthy = (
            status.get("system_resources", {}).get("system_memory_percent", 100) < 90 and
            status.get("llm_service_initialized", False)
        )
        
        return {
            "healthy": healthy,
            "status": status,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return {
            "healthy": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
