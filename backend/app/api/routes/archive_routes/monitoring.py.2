# In app/api/routes/monitoring.py
import os
import json
import logging
from pathlib import Path
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, timezone # Ensure timezone is imported

router = APIRouter()
logger = logging.getLogger(__name__)

LOG_DIR = Path("logs/pipeline") # Relative to app's working directory, e.g., /app/logs/pipeline
LOG_DIR.mkdir(parents=True, exist_ok=True) # Ensure log directory exists

@router.get("/pipelines", response_model=List[Dict[str, Any]])
async def get_pipelines():
    """Lists all available pipeline run IDs and their first event timestamp."""
    pipelines = []
    if not LOG_DIR.exists():
        return pipelines

    for log_file in LOG_DIR.glob("*.jsonl"):
        pipeline_id = log_file.stem
        first_event_time = None
        try:
            with open(log_file, "r") as f:
                first_line = f.readline()
                if first_line:
                    event = json.loads(first_line)
                    first_event_time = event.get("timestamp")
            pipelines.append({"id": pipeline_id, "start_time": first_event_time})
        except Exception as e:
            logger.error(f"Error reading or parsing log file {log_file}: {e}")
            pipelines.append({"id": pipeline_id, "start_time": "Error reading log"})
    
    pipelines.sort(key=lambda p: p.get("start_time") or "", reverse=True)
    return pipelines

@router.get("/pipelines/{pipeline_id}", response_model=List[Dict[str, Any]])
async def get_pipeline_details(pipeline_id: str):
    """Gets all events for a specific pipeline run."""
    log_file = LOG_DIR / f"{pipeline_id}.jsonl"
    if not log_file.exists():
        raise HTTPException(status_code=404, detail="Pipeline log not found")
    
    events = []
    try:
        with open(log_file, "r") as f:
            for line in f:
                try:
                    events.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.warning(f"Skipping malformed JSON line in {log_file}: {line.strip()} - Error: {e}")
        return events
    except Exception as e:
        logger.error(f"Error reading pipeline log {log_file}: {e}")
        raise HTTPException(status_code=500, detail=f"Error reading pipeline log: {e}")

@router.get("/stats", response_model=Dict[str, Any])
async def get_pipeline_stats():
    """Aggregates statistics from all pipeline runs."""
    stats = {
        "total_documents_processed": 0,
        "documents_processed_last_24h": 0,
        "documents_processed_last_7d": 0,
        "average_document_processing_time_ms": 0,
        "total_queries_processed": 0,
        "queries_processed_last_24h": 0,
        "queries_processed_last_7d": 0,
        "average_query_processing_time_ms": 0,
        "document_processing_errors": 0,
        "query_processing_errors": 0,
    }
    doc_processing_times = []
    query_processing_times = []
    now = datetime.now(timezone.utc)
    one_day_ago = now - timedelta(days=1)
    seven_days_ago = now - timedelta(days=7)

    if not LOG_DIR.exists():
        logger.warning("Log directory logs/pipeline not found.")
        return stats

    for log_file in LOG_DIR.glob("*.jsonl"):
        try:
            events_in_file = []
            with open(log_file, "r") as f:
                for line in f:
                    try:
                        event = json.loads(line)
                        events_in_file.append(event)
                    except json.JSONDecodeError:
                        logger.warning(f"Skipping malformed JSON line in {log_file.name} while populating events_in_file")
                        continue
            
            if not events_in_file:
                logger.warning(f"No valid events found in {log_file.name}. Skipping.")
                continue

            is_doc_pipeline = False
            is_query_pipeline = False
            pipeline_error = False
            pipeline_timestamp_str = None
            pipeline_processing_time_ms = None
            overall_event_found = False

            for event in reversed(events_in_file):
                stage = event.get("stage")
                if stage == "Overall Document Processing":
                    is_doc_pipeline = True
                    pipeline_timestamp_str = event.get("timestamp")
                    data = event.get("data", {})
                    if data.get("status") == "success":
                        pipeline_processing_time_ms = data.get("total_processing_time_ms")
                    elif data.get("status") == "error":
                        pipeline_error = True
                    overall_event_found = True
                    break
                elif stage == "Overall Query Processing":
                    is_query_pipeline = True
                    pipeline_timestamp_str = event.get("timestamp")
                    data = event.get("data", {})
                    if data.get("status") == "success":
                        pipeline_processing_time_ms = data.get("total_processing_time_ms")
                    elif data.get("status") == "error":
                        pipeline_error = True
                    overall_event_found = True
                    break
            
            if not overall_event_found:
                first_event = events_in_file[0]
                pipeline_timestamp_str = first_event.get("timestamp")
                if first_event.get("document_id") is not None:
                    is_doc_pipeline = True
                elif first_event.get("query_id") is not None:
                    is_query_pipeline = True
                else:
                    logger.warning(f"Could not determine pipeline type for {log_file.name} from first event. Skipping.")
                    continue
                pipeline_error = True # Assume error if overall status event is missing
                logger.warning(f"Overall event not found for {log_file.name}, using first event for timestamp and assuming error.")

            if not pipeline_timestamp_str:
                logger.warning(f"Could not determine timestamp for pipeline {log_file.name}. Skipping.")
                continue

            try:
                # Simplified and robust timestamp parsing
                parsed_dt = datetime.fromisoformat(pipeline_timestamp_str.replace("Z", "+00:00"))
                if parsed_dt.tzinfo is None:
                    dt_event = parsed_dt.replace(tzinfo=timezone.utc)
                else:
                    dt_event = parsed_dt.astimezone(timezone.utc) # Ensure it's UTC for comparison
            except ValueError as e:
                logger.error(f"Error parsing timestamp '{pipeline_timestamp_str}' from {log_file.name}: {e}. Skipping file.")
                continue

            if is_doc_pipeline:
                stats["total_documents_processed"] += 1
                if pipeline_error:
                    stats["document_processing_errors"] += 1
                if pipeline_processing_time_ms is not None:
                    try:
                        doc_processing_times.append(float(pipeline_processing_time_ms))
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid document processing time '{pipeline_processing_time_ms}' in {log_file.name}")
                if dt_event >= one_day_ago:
                    stats["documents_processed_last_24h"] += 1
                if dt_event >= seven_days_ago:
                    stats["documents_processed_last_7d"] += 1
            
            elif is_query_pipeline:
                stats["total_queries_processed"] += 1
                if pipeline_error:
                    stats["query_processing_errors"] += 1
                if pipeline_processing_time_ms is not None:
                    try:
                        query_processing_times.append(float(pipeline_processing_time_ms))
                    except (ValueError, TypeError):
                        logger.warning(f"Invalid query processing time '{pipeline_processing_time_ms}' in {log_file.name}")
                if dt_event >= one_day_ago:
                    stats["queries_processed_last_24h"] += 1
                if dt_event >= seven_days_ago:
                    stats["queries_processed_last_7d"] += 1

        except Exception as e:
            logger.error(f"Unhandled error processing log file {log_file.name}: {e}", exc_info=True)
            continue

    if doc_processing_times:
        stats["average_document_processing_time_ms"] = round(sum(doc_processing_times) / len(doc_processing_times), 2)
    if query_processing_times:
        stats["average_query_processing_time_ms"] = round(sum(query_processing_times) / len(query_processing_times), 2)
    
    logger.info(f"Calculated pipeline stats: {stats}")
    return stats

