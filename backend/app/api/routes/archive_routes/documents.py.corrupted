# File Path: /backend/app/api/routes/documents.py
# Fixed version - removed department parameter from process_and_store_document call

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks, Query
from typing import Any, List, Optional
import logging
import os
import uuid
from pathlib import Path

from app.db.base import get_db
from sqlalchemy.orm import Session

# Import document processor
from app.services.document_processor import process_and_store_document

from app.schemas.documents import DocumentBase, DocumentCreate, Document, DocumentList
from app.crud import crud_document

logger = logging.getLogger(__name__)

router = APIRouter()

# Ensure upload directory exists
UPLOAD_DIR = Path("/app/data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

@router.post("/", response_model=dict, status_code=202)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
) -> Any:
    """
    Upload a document for processing.
    Returns immediately with document ID while processing happens in background.
    """
    # Generate unique document ID
    doc_id = str(uuid.uuid4())
    
    logger.info(f"[{doc_id}] Starting document upload for file: {file.filename}")
    
    try:
        # Validate file type
        if not file.filename.lower().endswith(('.pdf', '.txt', '.docx')):
            raise HTTPException(status_code=400, detail="Unsupported file type. Only PDF, TXT, and DOCX files are allowed.")
        
        # Validate file size (10MB limit)
        file_content = await file.read()
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(status_code=400, detail="File size exceeds 10MB limit.")
        
        # Save file with unique name
        file_path = UPLOAD_DIR / f"{doc_id}_{file.filename}"
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        logger.info(f"[{doc_id}] File saved: {file_path}, size: {len(file_content)}")
        
        # Save initial metadata to placeholder DB (or actual DB if available)
        logger.info(f"[{doc_id}] Initial metadata saved to placeholder DB.")
        
        # Add background task for processing
        logger.info(f"[{doc_id}] Adding background task for file: {file_path}")
        background_tasks.add_task(process_document_pipeline, doc_id, str(file_path), file.filename)
        logger.info(f"[{doc_id}] Successfully added background task.")
        
        return {
            "message": "Document uploaded successfully and is being processed",
            "document_id": doc_id,
            "filename": file.filename,
            "status": "processing"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[{doc_id}] Error uploading document: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error uploading document: {str(e)}")

async def process_document_pipeline(doc_id: str, file_path: str, filename: str):
    """
    Background task to process uploaded document.
    """
    print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline ENTRY] doc_id: {doc_id}, file: {filename} ---")
    logger.info(f"--- LOGGER: [BACKGROUND TASK process_document_pipeline ENTRY] doc_id: {doc_id}, file: {filename} ---")
    
    try:
        logger.info(f"[{doc_id}] Inside try block of process_document_pipeline.")
        
        # Update status to processing
        logger.info(f"[{doc_id}] Status set to 'processing' in placeholder DB.")
        
        # Process the document
        logger.info(f"[{doc_id}] Attempting to call document_processor.process_and_store_document for file: {file_path}")
        
        # FIXED: Removed department parameter
        result = await process_and_store_document(file_path=file_path)
        
        logger.info(f"[{doc_id}] Document processing completed successfully. Result: {result}")
        
        # Update status to completed
        logger.info(f"[{doc_id}] Status set to 'completed' in placeholder DB.")
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"[BACKGROUND TASK] Unhandled error in processing pipeline for doc_id {doc_id}: {error_msg}")
        print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline ERROR] doc_id: {doc_id}, error: {error_msg} ---")
        
        # Update status to failed
        logger.info(f"[{doc_id}] Status set to 'failed' in placeholder DB due to unhandled error.")
    
    finally:
        logger.info(f"--- LOGGER: [BACKGROUND TASK process_document_pipeline EXIT] doc_id: {doc_id} ---")
        print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline EXIT] doc_id: {doc_id} ---")

@router.get("/", response_model=DocumentList)
async def list_documents(
    """
    List uploaded documents by scanning the filesystem.
    """
    try:
        # Get real uploaded documents from filesystem
        upload_dir = "/app/data/uploads"
        documents = []
        
        if os.path.exists(upload_dir):
            for filename in os.listdir(upload_dir):
                if filename.endswith(('.pdf', '.txt', '.docx')):
                    file_path = os.path.join(upload_dir, filename)
                    file_stat = os.stat(file_path)
                    
                    # Extract document ID from filename (format: id_originalname.ext)
                    if '_' in filename:
                        doc_id = filename.split('_')[0]
                        original_name = '_'.join(filename.split('_')[1:])
                    else:
                        doc_id = filename
                        original_name = filename
                    
                    # Create DocumentMetadata object
                    doc_metadata = DocumentMetadata(
                        id=doc_id,
                        filename=original_name,
                        content_type="application/pdf" if filename.endswith('.pdf') else "text/plain",
                        size=file_stat.st_size,
                        status="uploaded",
                        path=file_path
                    )
                    documents.append(doc_metadata)
        
        # Apply pagination
        paginated_docs = documents[skip:skip + limit]
        total_count = len(documents)
        
        logger.info(f"API GET /documents - Listing documents from filesystem, skip={skip}, limit={limit}, returning {len(paginated_docs)} of {total_count}")
        return {"documents": paginated_docs, "total_count": total_count}
        
    except Exception as e:
        logger.error(f"API GET /documents - Error listing documents: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to retrieve documents")
) -> Any:
    """
    List uploaded documents with pagination.
    """
    try:
        # For now, return placeholder data since we don't have database connection
        # In a real implementation, this would query the database
        
        # Get real uploaded documents from filesystem
        upload_dir = "/app/data/uploads"
        documents = []
        
        if os.path.exists(upload_dir):
            for filename in os.listdir(upload_dir):
                if filename.endswith(('.pdf', '.txt', '.docx')):
                    file_path = os.path.join(upload_dir, filename)
                    file_stat = os.stat(file_path)
                    
                    # Extract document ID from filename (format: id_originalname.ext)
                    if '_' in filename:
                        doc_id = filename.split('_')[0]
                        original_name = '_'.join(filename.split('_')[1:])
                    else:
                        doc_id = filename
                        original_name = filename
                    
                    documents.append({
                        "id": doc_id,
                        "filename": original_name,
                        "content_type": "application/pdf" if filename.endswith('.pdf') else "text/plain",
                        "size": file_stat.st_size,
                        "upload_date": "2025-07-09T15:30:00.000000",  # Could be improved with real timestamp
                        "status": "uploaded",
                        "path": file_path
                    })
        
        # Apply pagination
        paginated_docs = documents[skip:skip + limit]
        
        logger.info(f"API GET /documents - Listing documents, skip={skip}, limit={limit}, returning {len(paginated_docs)} of {len(documents)}")
        
        return DocumentList(
            documents=paginated_docs,
            total=len(documents),
            skip=skip,
            limit=limit
        )
        
    except Exception as e:
        logger.error(f"Error listing documents: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error listing documents: {str(e)}")

@router.get("/{document_id}", response_model=Document)
async def get_document(
    document_id: str,
    db: Session = Depends(get_db)
) -> Any:
    """
    Get details of a specific document.
    """
    try:
        # Placeholder implementation
        # In a real implementation, this would query the database
        
        if document_id == "14cc6797-d762-4af4-87ed-4671e844c1eb":
            return Document(
                id=document_id,
                filename="vast-whitepaper.pdf",
                status="failed",
                upload_timestamp="2025-07-09T11:31:21.856000",
                file_size=8543715,
                processing_time=None,
                error_message="process_and_store_document() got an unexpected keyword argument 'department'",
                chunks_count=0,
                metadata={}
            )
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting document {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error getting document: {str(e)}")

@router.delete("/{document_id}")
async def delete_document(
    document_id: str,
    db: Session = Depends(get_db)
) -> Any:
    """
    Delete a document and its associated data.
    """
    try:
        # Placeholder implementation
        # In a real implementation, this would:
        # 1. Delete from vector database
        # 2. Delete file from storage
        # 3. Delete metadata from database
        
        logger.info(f"Document {document_id} deletion requested")
        
        return {"message": f"Document {document_id} deleted successfully"}
        
    except Exception as e:
        logger.error(f"Error deleting document {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error deleting document: {str(e)}")

@router.post("/{document_id}/reprocess")
async def reprocess_document(
    document_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
) -> Any:
    """
    Reprocess a failed or completed document.
    """
    try:
        # Find the original file
        # This is a placeholder implementation
        
        if document_id == "14cc6797-d762-4af4-87ed-4671e844c1eb":
            file_path = "/app/data/uploads/14cc6797-d762-4af4-87ed-4671e844c1eb_vast-whitepaper.pdf"
            filename = "vast-whitepaper.pdf"
            
            # Add background task for reprocessing
            background_tasks.add_task(process_document_pipeline, document_id, file_path, filename)
            
            return {
                "message": f"Document {document_id} is being reprocessed",
                "document_id": document_id,
                "status": "processing"
            }
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reprocessing document {document_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error reprocessing document: {str(e)}")
