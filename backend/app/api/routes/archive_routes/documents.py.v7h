"""
Complete Corrected Documents Route - Final Version
Fixes both function signature and includes all necessary imports and schemas
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks, status, Form
from typing import List, Optional
import os
import uuid
import time
import logging
from pathlib import Path

# Import the document processor function
from app.services.document_processor import process_and_store_document

# Import corrected schemas
from pydantic import BaseModel, Field
from datetime import datetime

# CORRECTED SCHEMA - Matches field names used in route
class DocumentMetadata(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    filename: str
    department: str = "General"
    content_type: Optional[str] = None
    file_size: int
    upload_time: float = Field(default_factory=lambda: datetime.now().timestamp())
    status: str = "pending"
    path: Optional[str] = None
    error_message: Optional[str] = None

logger = logging.getLogger(__name__)

router = APIRouter()

# Global placeholder database for document metadata
documents_db = {}

# Valid departments
VALID_DEPARTMENTS = ["General", "IT", "HR", "Finance", "Legal"]

async def process_document_pipeline(doc_id: str, file_path: str, filename: str, department: str, content_type: str):
    """
    CORRECTED: Background task to process document with proper function signature
    Added content_type parameter to match actual function signature
    """
    print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline ENTRY] doc_id: {doc_id}, file: {filename}, department: {department} ---")
    logger.info(f"--- LOGGER: [BACKGROUND TASK process_document_pipeline ENTRY] doc_id: {doc_id}, file: {filename}, department: {department} ---")
    
    global documents_db
    processing_start_time = time.time()
    
    try:
        logger.info(f"[{doc_id}] Inside try block of process_document_pipeline.")
        
        # 1. Update Status to Processing
        if doc_id in documents_db:
            documents_db[doc_id]["status"] = "processing"
            documents_db[doc_id]["department"] = department
            logger.info(f"[{doc_id}] Status set to 'processing' and department set to '{department}' in placeholder DB.")
        else:
            logger.warning(f"[{doc_id}] doc_id not found in placeholder DB to set status to 'processing'.")

        # 2. CORRECTED: Call the function with the correct signature
        logger.info(f"[{doc_id}] Attempting to call document_processor.process_and_store_document for file: {file_path}")
        
        # The actual function signature is: process_and_store_document(file_path, content_type, db, department)
        # We need to provide a db connection - using None as placeholder for now
        db_connection = None  # TODO: Get actual database connection if needed
        
        try:
            # Try with all 4 parameters first
            await process_and_store_document(file_path, content_type, db_connection, department)
            logger.info(f"[{doc_id}] Successfully called process_and_store_document with 4 parameters")
        except TypeError as e:
            if "missing" in str(e) or "unexpected" in str(e):
                logger.info(f"[{doc_id}] 4-parameter call failed, trying alternative signatures...")
                
                # Try with 3 parameters (no db)
                try:
                    await process_and_store_document(file_path, content_type, department)
                    logger.info(f"[{doc_id}] Successfully called process_and_store_document with 3 parameters")
                except TypeError as e2:
                    # Try with 2 parameters (original signature)
                    try:
                        await process_and_store_document(file_path, department)
                        logger.info(f"[{doc_id}] Successfully called process_and_store_document with 2 parameters")
                    except TypeError as e3:
                        logger.error(f"[{doc_id}] All function signature attempts failed: {e3}")
                        raise e3
            else:
                raise e

        # 3. Update Status to Completed
        if doc_id in documents_db:
            documents_db[doc_id]["status"] = "completed"
            processing_time = time.time() - processing_start_time
            logger.info(f"[{doc_id}] Status set to 'completed' in placeholder DB. Processing time: {processing_time:.2f}s")
        else:
            logger.warning(f"[{doc_id}] doc_id not found in placeholder DB to set status to 'completed'.")

    except Exception as e:
        error_msg = f"[BACKGROUND TASK] Unhandled error in processing pipeline for doc_id {doc_id}: {e}"
        logger.error(error_msg, exc_info=True)
        print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline ERROR] doc_id: {doc_id}, error: {e} ---")
        
        if doc_id in documents_db:
            documents_db[doc_id]["status"] = "failed"
            documents_db[doc_id]["error_message"] = str(e)
            logger.info(f"[{doc_id}] Status set to 'failed' in placeholder DB due to unhandled error.")
        else:
            logger.warning(f"[{doc_id}] doc_id not found in placeholder DB to set status to 'failed' after unhandled error.")
    
    finally:
        logger.info(f"--- LOGGER: [BACKGROUND TASK process_document_pipeline EXIT] doc_id: {doc_id} ---")
        print(f"--- PRINT: [BACKGROUND TASK process_document_pipeline EXIT] doc_id: {doc_id} ---")

@router.post("/", response_model=DocumentMetadata, status_code=status.HTTP_202_ACCEPTED)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    department: str = Form(default="General")
):
    """
    CORRECTED: Upload document with proper function call parameters
    """
    logger.info(f"--- API POST /documents - Received file: {file.filename}, Department: {department} ---")
    
    # Validate department
    if department not in VALID_DEPARTMENTS:
        logger.warning(f"Invalid department '{department}', defaulting to 'General'")
        department = "General"
    
    # Generate unique document ID
    doc_id = str(uuid.uuid4())
    
    # Create upload directory if it doesn't exist
    upload_dir = Path("/app/data/uploads")
    upload_dir.mkdir(parents=True, exist_ok=True)
    
    # Save file with unique name
    file_extension = Path(file.filename).suffix
    unique_filename = f"{doc_id}_{file.filename}"
    file_path = upload_dir / unique_filename
    
    try:
        # Save uploaded file
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        file_size = len(content)
        upload_time = time.time()
        content_type = file.content_type or "application/octet-stream"
        logger.info(f"[{doc_id}] File saved: {file_path}, size: {file_size}")
        
        # Store metadata with matching field names
        documents_db[doc_id] = {
            "id": doc_id,
            "filename": file.filename,
            "department": department,
            "content_type": content_type,
            "file_size": file_size,
            "upload_time": upload_time,
            "status": "pending",
            "path": str(file_path),
            "error_message": None
        }
        
        logger.info(f"[{doc_id}] Initial metadata saved to placeholder DB with department: {department}.")
        
        # CORRECTED: Add background task with content_type parameter
        logger.info(f"[{doc_id}] Adding background task for file: {file_path}")
        background_tasks.add_task(
            process_document_pipeline,
            doc_id=doc_id,
            file_path=str(file_path),
            filename=file.filename,
            department=department,
            content_type=content_type  # ADDED: Pass content_type to background task
        )
        logger.info(f"[{doc_id}] Successfully added background task.")
        
        # Return document metadata
        return DocumentMetadata(
            id=doc_id,
            filename=file.filename,
            department=department,
            content_type=content_type,
            file_size=file_size,
            upload_time=upload_time,
            status="pending",
            path=str(file_path)
        )
        
    except Exception as e:
        logger.error(f"[{doc_id}] Error uploading document: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error uploading document: {str(e)}"
        )

@router.get("/", response_model=List[DocumentMetadata])
async def list_documents(skip: int = 0, limit: int = 100):
    """
    CORRECTED: List documents with proper field names
    """
    try:
        # Get documents from filesystem (existing logic)
        upload_dir = Path("/app/data/uploads")
        if not upload_dir.exists():
            logger.info("Upload directory does not exist, returning empty list")
            return []
        
        documents = []
        files = list(upload_dir.glob("*"))
        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        for file_path in files[skip:skip + limit]:
            if file_path.is_file():
                # Extract doc_id from filename (format: doc_id_original_filename)
                filename_parts = file_path.name.split('_', 1)
                if len(filename_parts) >= 2:
                    doc_id = filename_parts[0]
                    original_filename = filename_parts[1]
                else:
                    doc_id = str(uuid.uuid4())
                    original_filename = file_path.name
                
                # Get metadata from placeholder DB or create default
                if doc_id in documents_db:
                    doc_metadata = documents_db[doc_id]
                    status_val = doc_metadata.get("status", "unknown")
                    department = doc_metadata.get("department", "General")
                    upload_time = doc_metadata.get("upload_time", file_path.stat().st_mtime)
                    content_type = doc_metadata.get("content_type", "application/octet-stream")
                    file_size = doc_metadata.get("file_size", file_path.stat().st_size)
                else:
                    status_val = "unknown"
                    department = "General"
                    upload_time = file_path.stat().st_mtime
                    content_type = "application/octet-stream"
                    file_size = file_path.stat().st_size
                
                # Create DocumentMetadata with matching field names
                documents.append(DocumentMetadata(
                    id=doc_id,
                    filename=original_filename,
                    department=department,
                    content_type=content_type,
                    file_size=file_size,
                    upload_time=upload_time,
                    status=status_val,
                    path=str(file_path)
                ))
        
        logger.info(f"API GET /documents - Listing documents from filesystem, skip={skip}, limit={limit}, returning {len(documents)} of {len(files)}")
        return documents
        
    except Exception as e:
        logger.error(f"Error listing documents: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error listing documents: {str(e)}"
        )

@router.get("/departments")
async def get_departments():
    """
    Get list of available departments
    """
    return {
        "departments": VALID_DEPARTMENTS
    }
