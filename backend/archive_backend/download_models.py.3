import os
import shutil
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer

# Define cache directory within the container
CACHE_DIR = "/app/models_cache"
os.makedirs(CACHE_DIR, exist_ok=True)

# Get token from environment variable
hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
if not hf_token:
    print("Warning: HUGGING_FACE_HUB_TOKEN environment variable not set. Downloads for gated models may fail.")

# List of models to pre-download
llm_models = [
    "EleutherAI/gpt-j-6b",
    "meta-llama/Llama-2-7b-hf",
    "tiiuae/falcon-7b",
    "mistralai/Mistral-7B-v0.1"
]

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"

print(f"--- Starting model download to {CACHE_DIR} ---")

# Download LLMs and Tokenizers
for model_name in llm_models:
    print(f"Downloading {model_name}...")
    try:
        is_gated = "meta-llama" in model_name or "mistralai" in model_name
        auth_token = hf_token if is_gated else None
        
        # Download model, preferring safetensors and main revision
        print(f"Attempting to download model: {model_name}")
        AutoModelForCausalLM.from_pretrained(
            model_name, 
            cache_dir=CACHE_DIR, 
            token=auth_token, 
            revision="main", 
            use_safetensors=True # Prefer safetensors
        )
        print(f"Attempting to download tokenizer: {model_name}")
        # Download tokenizer from main revision
        AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=CACHE_DIR, 
            token=auth_token, 
            revision="main"
        )
        print(f"Successfully downloaded {model_name}.")
    except Exception as e:
        print(f"ERROR downloading {model_name}: {e}")

# Download Embedding Model
print(f"Downloading {embedding_model}...")
try:
    SentenceTransformer(embedding_model, cache_folder=CACHE_DIR)
    print(f"Successfully downloaded {embedding_model}.")
except Exception as e:
    print(f"ERROR downloading {embedding_model}: {e}")

print("--- Model download script finished ---")

# --- Cache Inspection and Pruning (Step 007 will refine this) ---
print(f"--- Starting Cache Inspection for {CACHE_DIR} ---")
total_size_before = 0
for dirpath, dirnames, filenames in os.walk(CACHE_DIR):
    for f in filenames:
        fp = os.path.join(dirpath, f)
        total_size_before += os.path.getsize(fp)
print(f"Total cache size before pruning: {total_size_before / (1024*1024*1024):.2f} GB")

# Example Pruning: Remove non-safetensors model files if safetensors exist
# This is a basic example; more sophisticated pruning might be needed.
print("--- Starting Basic Pruning --- (More specific pruning might be needed)")
# At this stage, we'll just log the size. Actual pruning logic will be more complex
# and might require knowing specific file names or patterns for non-essential files.
# For now, we are focusing on optimizing the download itself.

# More detailed inspection:
# List top-level directories in cache and their sizes
if os.path.exists(CACHE_DIR) and os.path.isdir(CACHE_DIR):
    for item in os.listdir(CACHE_DIR):
        item_path = os.path.join(CACHE_DIR, item)
        if os.path.isdir(item_path):
            dir_size = 0
            for dirpath, dirnames, filenames in os.walk(item_path):
                for f in filenames:
                    fp = os.path.join(dirpath, f)
                    dir_size += os.path.getsize(fp)
            print(f"Size of {item_path}: {dir_size / (1024*1024):.2f} MB")

print("--- Cache Inspection and Basic Pruning finished ---")

