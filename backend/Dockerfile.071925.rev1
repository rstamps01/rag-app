# check=skip=SecretsUsedInArgOrEnv
# File Path: /home/vastdata/rag-app/backend/Dockerfile
# Description: Dockerfile for the RAG application backend service.

# === Base Image ===
# Use NVIDIA CUDA 12.8 runtime image on Ubuntu 22.04 for RTX 5090 compatibility.
FROM nvidia/cuda:12.8.0-runtime-ubuntu22.04

ARG HUGGING_FACE_HUB_TOKEN
ENV HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# === Working Directory ===
WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-dev \
    build-essential \
    tesseract-ocr \
    tesseract-ocr-eng \
    libpq-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

RUN ln -s /usr/bin/python3 /usr/bin/python

COPY requirements.txt .

RUN pip3 install --no-cache-dir -r requirements.txt

RUN pip3 install --no-cache-dir \
    nvidia-cublas-cu12==12.8.4.1 \
    nvidia-cuda-cupti-cu12==12.8.90 \
    nvidia-cuda-nvrtc-cu12==12.8.93 \
    nvidia-cuda-runtime-cu12==12.8.90 \
    nvidia-cudnn-cu12==9.10.2.21 \
    nvidia-cufft-cu12==11.3.3.83 \
    nvidia-cufile-cu12==1.13.1.3 \
    nvidia-curand-cu12==10.3.9.90 \
    nvidia-cusolver-cu12==11.7.3.90 \
    nvidia-cusparse-cu12==12.5.8.93 \
    nvidia-cusparselt-cu12==0.7.1 \
    nvidia-nccl-cu12==2.27.3 \
    nvidia-nvjitlink-cu12==12.8.93 \
    nvidia-nvshmem-cu12==3.3.9 \
    nvidia-nvtx-cu12==12.8.90 \
    --index-url https://download.pytorch.org/whl/nightly/cu128

RUN pip3 install --no-cache-dir \
    torch==2.9.0.dev20250708+cu128 \
    torchvision==0.24.0.dev20250708+cu128 \
    pytorch-triton==3.4.0+gitae848267 \
    --index-url https://download.pytorch.org/whl/nightly/cu128

RUN mkdir -p /app/data/uploads /app/data/logs /app/models_cache /app/models_cache/datasets /app/models_cache/transformers /app/models_cache/home /app/models_cache/hub

## Alternate Cached Deployment Model
#COPY download_models.py .
#RUN python3 download_models.py
#ENV HF_HOME=/app/models_cache/home
#COPY models_cache/ /app/models_cache
#COPY app/ ./app/
#EXPOSE 8000
#CMD ["python3", "/app/scripts/initialize_model_cache.py"]



### Diabled for 7/19/2025 Testing cache model
#COPY app/ ./app/
#EXPOSE 8000
#CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

### Added for 7/17/2025 Testing cahe model

# Copy cache utilities and initialization scripts
COPY scripts/model_cache_utils.py /app/scripts/
COPY scripts/initialize_model_cache.py /app/scripts/

# Make scripts executable
RUN chmod +x /app/scripts/*.py

# Set up cache environment variables
ENV HF_HOME=/app/models_cache
ENV TRANSFORMERS_CACHE=/app/models_cache/transformers
ENV HF_DATASETS_CACHE=/app/models_cache/datasets
ENV HF_HUB_CACHE=/app/models_cache/hub
ENV MODELS_CACHE_DIR=/app/models_cache

CMD ["python3", "/app/scripts/initialize_model_cache.py"]

#HOST_HF_CACHE=/app/models_cach

# === System Dependencies ===
# Install system dependencies
# NOTE - Added --no-install-recommends as seen in rag-app-05
# This reduces the size of the image by not installing unnecessary packages.
# It is important to keep the image size small for faster deployments and better performance.
# The packages installed here are essential for running Python, building Python packages, and OCR functionality.
# The tesseract-ocr package is used for Optical Character Recognition (OCR) tasks.
# The libpq-dev package is required for PostgreSQL database connectivity.

# Install Python packages listed in requirements.txt.
# RUN pip3 install --no-cache-dir -r requirements.txt
# RUN pip3 install -r requirements.txt

# === PyTorch Installation for CUDA 12.8 ===
# === Upddated Pytorch Installation for CUDA 12.8 - July 8, 2025 ===
# Install PyTorch and torchvision compatible with CUDA 12.8. #   Removed torchaudio==2.8.0.dev20250708+cu128
# This is done separately to ensure the correct CUDA version is used.

# RUN pip3 install --no-cache-dir \
#     pillow==11.2.1 \
#     --index-url https://download.pytorch.org/whl/nightly/cu128
# === Python Dependencies ===
# Copy only the requirements file first to leverage Docker cache.

#COPY requirements.txt .

# Install Python packages listed in requirements.txt.
#RUN pip3 install --no-cache-dir -r requirements.txt

# === Pre-download Models ===
# Accept Hugging Face token as a build argument
# ARG HUGGINGFACE_TOKEN

#ARG HUGGING_FACE_HUB_TOKEN
#ENV HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}

# Create necessary directories
# RUN mkdir -p /app/data/uploads /app/data/logs /app/models_cache /app/models_cache/home

# Copy and run the script to download models during build time.
# COPY download_models.py .
# RUN python3 download_models.py

#### Potential Change to Local
# ENV TRANSFORMERS_CACHE=/app/models_cache
#ENV HF_HOME=/app/models_cache/home
#COPY models_cache/ /app/models_cache

# === Application Code ===
# Copy the entire application code into the container.
# This includes the core logic, API routes (including monitoring), etc.
# COPY app/ ./app/

# === Port Exposure ===
# Expose the port the FastAPI application will run on.
# EXPOSE 8000

# === Run Command ===
# Command to start the Uvicorn server for the FastAPI application.
# Runs on all interfaces (0.0.0.0) inside the container.
# CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]